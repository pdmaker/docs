---
title: LangChain
description: 使用 LangChain 框架接入 API易 的完整指南
---

# LangChain 集成指南

LangChain 是一个强大的框架，用于开发基于语言模型的应用程序。通过 API易，您可以在 LangChain 中使用各种主流 AI 模型。

## 安装准备

首先确保已安装 LangChain：

```bash
pip install langchain langchain-openai
```

## 配置方式

### 方式一：环境变量配置（推荐）

```python
import os

# 设置环境变量
os.environ["OPENAI_API_KEY"] = "您的API易密钥"
os.environ["OPENAI_BASE_URL"] = "https://api.apiyi.com/v1"  # 注意：必须以 /v1 结尾
```

### 方式二：直接传递参数

```python
from langchain_openai import ChatOpenAI

# 创建模型实例时直接传递参数
llm = ChatOpenAI(
    api_key="您的API易密钥",
    base_url="https://api.apiyi.com/v1",  # 注意：必须以 /v1 结尾
    model="gpt-3.5-turbo"
)
```

## 使用示例

### 基础对话

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

# 初始化模型
llm = ChatOpenAI(
    api_key="您的API易密钥",
    base_url="https://api.apiyi.com/v1",
    model="gpt-3.5-turbo",
    temperature=0.7
)

# 创建消息
messages = [
    SystemMessage(content="你是一个有帮助的助手"),
    HumanMessage(content="请介绍一下 Python 的主要特点")
]

# 获取响应
response = llm.invoke(messages)
print(response.content)
```

### 流式输出

```python
from langchain_openai import ChatOpenAI
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# 配置流式输出
llm = ChatOpenAI(
    api_key="您的API易密钥",
    base_url="https://api.apiyi.com/v1",
    model="gpt-4",
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()]
)

# 流式生成响应
llm.invoke("写一首关于春天的诗")
```

### 使用不同模型

```python
# GPT-4 模型
gpt4 = ChatOpenAI(
    api_key="您的API易密钥",
    base_url="https://api.apiyi.com/v1",
    model="gpt-4"
)

# Claude 模型
claude = ChatOpenAI(
    api_key="您的API易密钥",
    base_url="https://api.apiyi.com/v1",
    model="claude-3-opus-20240229"
)

# 使用不同模型
gpt4_response = gpt4.invoke("解释量子计算")
claude_response = claude.invoke("解释量子计算")
```

## 高级用法

### 构建对话链

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# 创建带记忆的对话链
memory = ConversationBufferMemory()
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# 进行多轮对话
conversation.predict(input="你好，我想学习机器学习")
conversation.predict(input="你能推荐一些入门资源吗？")
conversation.predict(input="这些资源的难度如何？")
```

### 文档问答系统

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA

# 配置嵌入模型
embeddings = OpenAIEmbeddings(
    api_key="您的API易密钥",
    base_url="https://api.apiyi.com/v1"
)

# 加载和处理文档
loader = TextLoader("document.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# 创建向量存储
vectorstore = FAISS.from_documents(texts, embeddings)

# 创建问答链
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# 提问
query = "文档中提到了什么重要概念？"
result = qa.run(query)
print(result)
```

### 使用 Agent

```python
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain.tools import Tool
from langchain import hub

# 定义工具
def get_weather(location: str) -> str:
    """获取指定地点的天气"""
    return f"{location}的天气是晴天，温度25°C"

weather_tool = Tool(
    name="Weather",
    func=get_weather,
    description="获取指定地点的天气信息"
)

# 创建 Agent
prompt = hub.pull("hwchase17/openai-functions-agent")
agent = create_openai_functions_agent(llm, [weather_tool], prompt)
agent_executor = AgentExecutor(agent=agent, tools=[weather_tool], verbose=True)

# 使用 Agent
result = agent_executor.invoke({"input": "北京今天的天气怎么样？"})
print(result["output"])
```

## 性能优化

### 批量处理

```python
# 批量生成响应
prompts = [
    "解释人工智能",
    "什么是机器学习",
    "深度学习的应用"
]

responses = llm.batch([HumanMessage(content=p) for p in prompts])
for response in responses:
    print(response.content)
    print("-" * 50)
```

### 异步调用

```python
import asyncio

async def async_generate():
    # 异步生成响应
    response = await llm.ainvoke("异步生成的内容")
    return response.content

# 运行异步函数
result = asyncio.run(async_generate())
print(result)
```

## 错误处理

```python
from langchain.callbacks import get_openai_callback

try:
    # 使用回调监控 API 调用
    with get_openai_callback() as cb:
        response = llm.invoke("你好")
        print(f"使用的 Token 数: {cb.total_tokens}")
        print(f"API 调用成本: ${cb.total_cost}")
except Exception as e:
    print(f"发生错误: {e}")
    # 处理 API 错误、网络错误等
```

## 最佳实践

1. **API 密钥安全**：不要在代码中硬编码 API 密钥，使用环境变量或配置文件
2. **错误重试**：实现重试机制处理临时网络问题
3. **模型选择**：根据任务复杂度选择合适的模型，平衡成本和效果
4. **上下文管理**：合理控制对话历史长度，避免超出模型限制
5. **并发控制**：使用异步或批量处理提高效率

## 注意事项

- **URL 格式**：base_url 必须以 `/v1` 结尾
- **模型名称**：使用 API易 支持的模型名称
- **Token 限制**：注意不同模型的 token 限制
- **费用监控**：定期检查 API 使用情况和费用

需要更多帮助？请访问 [API易文档中心](https://api.apiyi.com) 或联系技术支持。